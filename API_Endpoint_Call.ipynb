{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMShjXHQKYGIlX2YMcQZuKG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paraskevi-KIvroglou/hotel-assistant-project/blob/main/API_Endpoint_Call.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUKjfo0pKzVB",
        "outputId": "d865df19-75d1-4fe4-e565-ad53703aef96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iaria.com\n",
            "I'd be happy to help! Could you please provide me with your travel dates and the number of guests who will be staying in the room? That way I can give you an accurate quote.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "API_URL = \"https://kp4xdy196cw81uf3.us-east-1.aws.endpoints.huggingface.cloud\"\n",
        "headers = {\n",
        "\t\"Accept\" : \"application/json\",\n",
        "\t\"Authorization\": \"Bearer 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\",\n",
        "\t\"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "def generate_response(text):\n",
        "  input = {\n",
        "    \"inputs\": text,\n",
        "    \"parameters\": {\n",
        "      \"max_new_tokens\" : 128,\n",
        "      \"top_k\": 10,\n",
        "      \"top_p\": 0.95,\n",
        "      \"typical_p\": 0.95,\n",
        "      \"temperature\": 0.01,\n",
        "      \"repetition_penalty\": 1.03,\n",
        "      \"stop\" : [\"/nHuman:\", \"/nUser:\", \"<end of message>\\n\"]\n",
        "    }\n",
        "  }\n",
        "\n",
        "  output = query(input)\n",
        "  response = output[0][\"generated_text\"]\n",
        "  response = response.replace(text, \"\")\n",
        "  splitResponse = response.split(\"Human:\")\n",
        "  return splitResponse[0]\n",
        "\n",
        "\n",
        "def format_chat_prompt(message, chat_history):\n",
        "    prompt = \"\"\" Do not repeat questions and do not generate answer for user/human.\n",
        "\n",
        "    You are a helpful hotel booking asssitant.\n",
        "    Below is an instruction that describes a task.\n",
        "    Write a response that appropriately completes the request.\n",
        "    Reply with the most helpful and logic answer. During the conversation you need to ask the user\n",
        "    the following questions to complete the hotel booking task.\n",
        "    1) Where would you like to stay and when?\n",
        "    2) How many people are staying in the room?\n",
        "    3) Do you prefer any ammenities like breakfast included or gym?\n",
        "    4) What is your name, your email address and phone number?\n",
        "    When name, email and phone number is provided, you, a hotel booking assistant, shouldd respond with a thankful answer.\n",
        "    When the user says please book the room or Yes, book it, you should repsond with : \"Yes,I booked the rooom.\"\n",
        "    \"\"\"\n",
        "    for turn in chat_history:\n",
        "        user_message, bot_message = turn\n",
        "        prompt = f\"{prompt}\\nHuman: {user_message}\\nAssistant: {bot_message}\"\n",
        "    prompt = f\"{prompt}\\nHuman: {message}\\nAssistant:\"\n",
        "    return prompt\n",
        "\n",
        "#print(generate_response(\"Hello I would like to book a hotel room.AI:\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWPwyIIjMEEI",
        "outputId": "7de58bef-a1f8-484b-c259-2ee1f7cde526"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.3/316.3 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat_output(message, history):\n",
        "    print(message)\n",
        "    prompt = format_chat_prompt(message,history)\n",
        "    result = generate_response(text = prompt)\n",
        "    return result\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    #chatbot_component = gr.Chatbot(height=300, label = \"history\")\n",
        "    #textbox_component = gr.Textbox(placeholder=\"Can I help you to book a hotel?\", container=False, label = \"input\", scale=7)\n",
        "\n",
        "    demo.chatbot_interface = gr.ChatInterface(\n",
        "        fn=chat_output,\n",
        "        examples = [\"Hello I would like to book a hotel room.\", \"Hello I want to stay in Nuremberg in 30th of May until 1st of June.\" , \"2 people, me and my partner.\",\n",
        "                    \"2 people, me and my partner. I would like a breakfast included.\", \" I would like a breakfast included.\", \"Just me.I am travelling alone.\"],\n",
        "        #outputs=chatbot_component,\n",
        "        title = \"Hotel Booking Assistant Chat 🤗\",\n",
        "        description = \"I am your hotel booking assistant. Feel free to start chatting with me.\"\n",
        "    )\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "cbkEyKvOMDBn",
        "outputId": "5de0f63a-8b04-46d3-de6f-d15665467f01"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://870cf5d79e2835c054.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://870cf5d79e2835c054.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://870cf5d79e2835c054.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}